# 1. 고유값과 고유벡터

## 고유벡터와 고유값

고유하다는 것은 상황이 변화해도 그 특성을 잃지 않는 것을 의미한다. 그럼, 벡터가 고유하다는 것은 무엇일까? 벡터가 어떠한 상황에서도 그 특성, 즉 방향성을 잃지 않는 것을 의미한다. 즉, 고유벡터란 **선형변환 이후에도 변환 결과가 자신의 상수배를 한 결과일 때의 벡터**를 의미한다. 선형 변환이란 쉽게 말해 어떤 행렬을 벡터에 곱하는 것이다.

<details>
    <summary> 여러가지 선형변환들 </summary>

**선형변환이란?**

벡터에 어떠한 행렬을 곱하는 것을 '벡터에 행렬을 통과시킨다'라고 표현한다. 아무튼, 벡터에 어떠한 행렬을 곱하게 되면 벡터의 크기와 방향이 변한다.

하지만, 아무리 벡터의 크기나 방향이 변해 봤자, 조금 더 벡터의 크기가 커진다거나, 벡터의 방향이 치우치는 정도에 그치게 된다. 벡터가 곡선의 형상을 띄거나 하지는 않는다는 것이다. 이를 '선형적으로 변화했다'라고 말한다.

이처럼, 벡터가 어떠한 행렬을 통과하여 선형적인 변화를 일으키게 하는 것을 **선형변환 (linear transformation)** 이라고 한다.


선형변환에 속하는 다양한 변환들이 있으며, 모든 변환들은 행렬을 곱하여 이뤄진다. 어찌보면, 벡터에 곱해지는 행렬이 곧 함수와도 같은 역할을 한다고 볼 수 있다. 이런 변환들은 주로 컴퓨터 그래픽에 많이 활용된다. 하지만 아핀변환이라는 것은 LLM에서도 자주 언급되긴 한다.


---

▶️**scailing (비례변환)**  비례변환은 벡터의 방향과 크기가 변화하는 변환을 의미한다.

▶️**Rotation (회전변환)**  회전변환은 좌표평면이 원점을 중심으로 회전하는 것을 의미한다. 회전변환된 벡터는 원래의 벡터와 선형독립이며, 회전변환시 고유값과 고유벡터는 존재하지 않는다. 

▶️**Shearing (전단변환)** 전단변환은 특정 차원값에만 변화를 주는 변환을 의미한다. 기하학적으로 이해하면, y축을 고정하고 x축 방향으로만 변화를 가하는 것을 의미한다.

앞서 살펴본 변환들은 모두 원점이 변화하지 않는 변환이다. 원점을 이동시키는 변환도 있다. 바로 **이동변환** 이라는 것인데, 대표적으로 **아핀변환(Affine)** 이 있다.

▶️**Affine (아핀변환)** 아핀변환은 선형변환과 이동변환이 결합된 변환으로, 원점을 이동시키면서 벡터의 방향과 크기도 변화시키는 변환이다.

</details>


식으로 나타내면 다음과 같이 표현할 수 있다.

$$
Xv = \lambda v
$$

여기서 $$\lambda$$ 는 벡터에 곱해지는 스칼라를 의미한다. 이 스칼라 값인 람다를 **고유값**이라고 칭한다. 즉, 고유값이란 고유벡터에 곱해지는 상수값을 의미한다.

고유벡터는 하나만 존재할 수도, 무한하게 존재할 수도 있으며, 고유 벡터가 아예 존재하지 않을 수 있다. 다음은 고유벡터를 구하는 과정을 나타낸 식이다.

$$
Av = \lambda v
\newline
\newline
Av - \lambda v = 0
\newline
(A- \lambda I)v = 0
\newline
v \neq 0, \space  (A-\lambda I) = 0
\newline
eigen\space vector = v \forall N(A - \lambda I)
$$

고유벡터가 무한히 존재하는 경우, 보통 해당 벡터의 basis를 고유벡터로 삼는다. 고유벡터가 무한히 존재하는 경우는 보통 행렬식이 0, 즉, 곱해지는 행렬이 선형 종속이라 1차원에서 span하는 경우이다. 이런 케이스를 시각적으로 보면, 좌표 평면이 일직선으로 짜부라드는 것을 확인할 수 있다.

---

## 고유값 분해 (eigenvalue decomposition)

고유값 분해는 말 그대로 어떤 벡터를 스칼라(고유값)과 고유 벡터로 나누는 것을 의미한다.

$$\lambda$$ 값은 여러 개 존재할 수 있으며, 대각행렬로 lambda 값들을 표현할 수 있다. 대각행렬로 나타낸 람다값은 대문자 람다 $$\Lambda$$로 나타낸다.

$$AV = V \Lambda$$ 로 나타낼 수 있는데, 고윳값을 갖는 모든 벡터는 Invertable 하다는 성질을 활용해 식을 정리하면

$$
A = V \Lambda  V^{-1}
$$ 
로 정리할 수 있다.

이번에는 $$\Lambda$$만 남도록 식을 정리해보자. 마찬가지로, V의 invertable한 성질을 활용하도록 한다. 

$$
V^{-1}A = V^{-1}V\Lambda V^{-1}
\newline
V^{-1}AV = V^{-1}\Lambda V^{-1}V
\newline
V^{-1}AV = \Lambda
$$


다음은 고유값 분해와 관련된 특성들이다.

1. $$A^T$$의 고유값은 A의 고유값과 같다.

2. A가 orthogonal matrix이면, $$\lambda = +-1$$ 이다.

3. A가 Postivie Semi Definite (PSD) 이면 $$\lambda$$는 무조건 0보다 크거나 같다.

4. Diagnal Matrix의 Non-Zero 고유값의 수는 rank와 동일하다.

5. Symmetric Matrix는 무조건 Diagnalizable 하며 (역 성립 X), 따라서 $$ A = Q\Lambda Q^T$$ 된다. 


고유값 분해로 얻을 수 있는 장점은 무엇일까?

1. 행렬의 거듭제곱 계산이 용이해진다

2. 고유값 분해를 통해 쉽게 Inverse Matrix를 얻을 수 있다

3. 행렬식을 구하기 쉽다

<details>
    <summary>4. trace(대각합)값을 구하기 쉽다</summary>
    $tr(A)$는 고유값과 고유벡터로 나타내면 $tr(V\LambdaV)$ 이다.
</details>

5. rank-deficient는 곧 행렬식이 0임을 의미하고, 행렬식이 0이란 것은 곧 0인 고유값이 하나 이상 존재함을 알 수 있다.



